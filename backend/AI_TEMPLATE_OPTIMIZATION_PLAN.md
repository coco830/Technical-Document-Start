# AI æ¨¡æ¿ç”Ÿæˆç³»ç»Ÿ - ä¼˜åŒ–å»ºè®®ä¸æ”¹è¿›æ–¹æ¡ˆ

ç‰ˆæœ¬ï¼šv1.0
æ—¥æœŸï¼š2025-11-04
ä½œè€…ï¼šAI Assistant

---

## ä¸€ã€å·²å®ŒæˆåŠŸèƒ½æ€»ç»“

### âœ… æ ¸å¿ƒåŠŸèƒ½
1. **æ¨¡æ¿ç®¡ç†ç³»ç»Ÿ**
   - YAML é…ç½®æ–‡ä»¶æ³¨å†Œè¡¨
   - æ¨¡æ¿åŠ¨æ€åŠ è½½ä¸ç¼“å­˜
   - å¤šæ¨¡æ¿æ”¯æŒï¼ˆå½“å‰ï¼šåº”æ€¥é¢„æ¡ˆæ¨¡æ¿ï¼‰

2. **å®‰å…¨é˜²æŠ¤æœºåˆ¶**
   - Jinja2 æ²™ç®±æ¨¡å¼
   - è¾“å…¥éªŒè¯ï¼ˆç±»å‹ã€å­—æ®µåã€å±é™©å…³é”®å­—ï¼‰
   - è¾“å‡ºæ¸…ç†ï¼ˆXSS é˜²æŠ¤ï¼‰
   - æ–‡ä»¶å¤§å°é™åˆ¶
   - è¯·æ±‚é™æµ

3. **API æ¥å£**
   - `GET /api/ai/templates` - åˆ—å‡ºæ‰€æœ‰æ¨¡æ¿
   - `GET /api/ai/templates/{id}/schema` - è·å–æ¨¡æ¿ç»“æ„
   - `POST /api/ai/generate` - ç”Ÿæˆå†…å®¹
   - `DELETE /api/ai/cache` - æ¸…é™¤ç¼“å­˜

4. **ç¼“å­˜æœºåˆ¶**
   - å†…å­˜ç¼“å­˜
   - MD5 å“ˆå¸Œé”®
   - TTL è¿‡æœŸï¼ˆ1å°æ—¶ï¼‰

---

## äºŒã€çŸ­æœŸä¼˜åŒ–å»ºè®®ï¼ˆ1-2å‘¨ï¼‰

### ğŸ”§ ä¼˜å…ˆçº§ P0ï¼šç”Ÿäº§ç¯å¢ƒå°±ç»ª

#### 1. æ¥å…¥çœŸå® AI æ¨¡å‹
**ç›®æ ‡**ï¼šæ›¿æ¢æ¨¡æ‹Ÿç”Ÿæˆï¼Œæ¥å…¥ OpenAI/Claude/é€šä¹‰åƒé—®ç­‰

**å®æ–½æ­¥éª¤**ï¼š
```python
# 1. å®‰è£… SDK
# requirements.txt
openai==1.12.0  # æˆ– anthropic==0.18.0

# 2. é…ç½®ç¯å¢ƒå˜é‡
# .env
OPENAI_API_KEY=sk-...
OPENAI_BASE_URL=https://api.openai.com/v1
AI_MODEL=gpt-4-turbo-preview

# 3. å®ç° AI æœåŠ¡
# backend/app/services/ai_service.py
from openai import OpenAI
import os

class AIService:
    def __init__(self):
        self.client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_BASE_URL")
        )

    def generate(self, prompt: str, config: dict) -> str:
        response = self.client.chat.completions.create(
            model=config.get("model", "gpt-4"),
            messages=[{"role": "user", "content": prompt}],
            temperature=config.get("temperature", 0.7),
            max_tokens=config.get("max_tokens", 2000)
        )
        return response.choices[0].message.content

# 4. åœ¨ ai_generate.py ä¸­é›†æˆ
from ..services.ai_service import AIService

ai_service = AIService()
generated_content = ai_service.generate(prompt, ai_config)
```

**é¢„ä¼°å·¥ä½œé‡**ï¼š4-6 å°æ—¶

---

#### 2. é›†æˆ Redis ç¼“å­˜
**ç›®æ ‡**ï¼šæ›¿æ¢å†…å­˜ç¼“å­˜ï¼Œæ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²

**å®æ–½æ­¥éª¤**ï¼š
```python
# 1. å®‰è£…ä¾èµ–
# requirements.txt
redis==5.0.1

# 2. é…ç½®ç¯å¢ƒå˜é‡
# .env
REDIS_URL=redis://localhost:6379/0

# 3. å®ç° Redis ç¼“å­˜æœåŠ¡
# backend/app/services/cache_service.py
import redis
import json
from typing import Optional

class CacheService:
    def __init__(self, redis_url: str):
        self.client = redis.from_url(redis_url)

    def get(self, key: str) -> Optional[dict]:
        data = self.client.get(key)
        if data:
            return json.loads(data)
        return None

    def set(self, key: str, value: dict, ttl: int = 3600):
        self.client.setex(
            key,
            ttl,
            json.dumps(value, ensure_ascii=False)
        )

    def delete(self, key: str):
        self.client.delete(key)

    def clear_pattern(self, pattern: str):
        """æ¸…é™¤åŒ¹é…æ¨¡å¼çš„æ‰€æœ‰é”®"""
        for key in self.client.scan_iter(match=pattern):
            self.client.delete(key)

# 4. åœ¨ ai_generate.py ä¸­ä½¿ç”¨
cache_service = CacheService(os.getenv("REDIS_URL"))

# æ£€æŸ¥ç¼“å­˜
cached = cache_service.get(cache_key)
if cached:
    return GenerationResponse(success=True, content=cached["content"], cached=True)

# ä¿å­˜ç¼“å­˜
cache_service.set(cache_key, {"content": generated_content}, ttl)
```

**é¢„ä¼°å·¥ä½œé‡**ï¼š3-4 å°æ—¶

---

#### 3. æ·»åŠ é‡è¯•æœºåˆ¶
**ç›®æ ‡**ï¼šæé«˜ AI API è°ƒç”¨ç¨³å®šæ€§

**å®æ–½æ­¥éª¤**ï¼š
```python
# 1. å®‰è£…ä¾èµ–
# requirements.txt
tenacity==8.2.3

# 2. å®ç°é‡è¯•è£…é¥°å™¨
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from openai import OpenAIError

class AIService:
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((OpenAIError, TimeoutError)),
        reraise=True
    )
    def generate(self, prompt: str, config: dict) -> str:
        try:
            response = self.client.chat.completions.create(
                model=config.get("model", "gpt-4"),
                messages=[{"role": "user", "content": prompt}],
                temperature=config.get("temperature", 0.7),
                max_tokens=config.get("max_tokens", 2000),
                timeout=30  # 30 ç§’è¶…æ—¶
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"AI ç”Ÿæˆå¤±è´¥: {e}")
            raise
```

**é¢„ä¼°å·¥ä½œé‡**ï¼š2 å°æ—¶

---

### ğŸ”§ ä¼˜å…ˆçº§ P1ï¼šåŠŸèƒ½å¢å¼º

#### 4. å®ç°æµå¼å“åº”
**ç›®æ ‡**ï¼šæå‡ç”¨æˆ·ä½“éªŒï¼Œå®æ—¶æ˜¾ç¤ºç”Ÿæˆå†…å®¹

```python
from fastapi.responses import StreamingResponse
from openai import OpenAI

@router.post("/api/ai/generate/stream")
async def stream_generate(
    request: GenerationRequest,
    current_user: User = Depends(get_current_user)
):
    """æµå¼ç”Ÿæˆå†…å®¹"""

    async def generate_stream():
        # æ¸²æŸ“ prompt
        prompt = template_loader.render_prompt(...)

        # æµå¼è°ƒç”¨ OpenAI
        stream = ai_service.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            stream=True
        )

        for chunk in stream:
            if chunk.choices[0].delta.content:
                yield f"data: {json.dumps({'content': chunk.choices[0].delta.content})}\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream"
    )
```

**é¢„ä¼°å·¥ä½œé‡**ï¼š4 å°æ—¶

---

#### 5. æ‰¹é‡ç”Ÿæˆæ¥å£
**ç›®æ ‡**ï¼šæ”¯æŒä¸€æ¬¡ç”Ÿæˆå¤šä¸ªç« èŠ‚

```python
@router.post("/api/ai/generate/batch")
async def batch_generate(
    requests: List[GenerationRequest],
    current_user: User = Depends(get_current_user)
):
    """æ‰¹é‡ç”Ÿæˆå†…å®¹"""
    import asyncio

    async def generate_single(req: GenerationRequest):
        try:
            return await generate_content(req, current_user)
        except Exception as e:
            return GenerationResponse(success=False, error=str(e))

    # å¹¶å‘ç”Ÿæˆ
    results = await asyncio.gather(
        *[generate_single(req) for req in requests],
        return_exceptions=False
    )

    return {
        "success": True,
        "results": results,
        "total": len(requests),
        "succeeded": sum(1 for r in results if r.success)
    }
```

**é¢„ä¼°å·¥ä½œé‡**ï¼š3 å°æ—¶

---

## ä¸‰ã€ä¸­æœŸä¼˜åŒ–å»ºè®®ï¼ˆ1ä¸ªæœˆï¼‰

### ğŸ“Š æ—¥å¿—ä¸ç›‘æ§

#### 6. å®Œå–„å®¡è®¡æ—¥å¿—
```python
import logging
from datetime import datetime

# åˆ›å»ºå®¡è®¡æ—¥å¿—è®°å½•å™¨
audit_logger = logging.getLogger("audit")
audit_logger.setLevel(logging.INFO)

# æ–‡ä»¶å¤„ç†å™¨
handler = logging.FileHandler("logs/audit.log")
handler.setFormatter(logging.Formatter(
    '%(asctime)s | %(message)s'
))
audit_logger.addHandler(handler)

# è®°å½•å®¡è®¡äº‹ä»¶
def log_ai_generation(user_id, template_id, section_id, success, duration):
    audit_logger.info(json.dumps({
        "timestamp": datetime.now().isoformat(),
        "event": "ai_generate",
        "user_id": user_id,
        "template_id": template_id,
        "section_id": section_id,
        "success": success,
        "duration_ms": duration,
        "ip_address": request.client.host
    }, ensure_ascii=False))
```

---

#### 7. Prometheus ç›‘æ§æŒ‡æ ‡
```python
from prometheus_client import Counter, Histogram, Gauge

# å®šä¹‰æŒ‡æ ‡
ai_requests_total = Counter(
    'ai_requests_total',
    'Total AI generation requests',
    ['template_id', 'section_id', 'status']
)

ai_request_duration = Histogram(
    'ai_request_duration_seconds',
    'AI request duration in seconds',
    ['template_id']
)

cache_hit_rate = Gauge(
    'cache_hit_rate',
    'Cache hit rate',
    ['template_id']
)

# ä½¿ç”¨æŒ‡æ ‡
@router.post("/api/ai/generate")
async def generate_content(...):
    start_time = time.time()

    try:
        # ... ç”Ÿæˆé€»è¾‘ ...
        ai_requests_total.labels(
            template_id=template_id,
            section_id=section_id,
            status="success"
        ).inc()
    except Exception as e:
        ai_requests_total.labels(
            template_id=template_id,
            section_id=section_id,
            status="error"
        ).inc()
        raise
    finally:
        duration = time.time() - start_time
        ai_request_duration.labels(template_id=template_id).observe(duration)
```

---

### ğŸ” å®‰å…¨å¢å¼º

#### 8. RBAC æƒé™æ§åˆ¶
```python
# å®šä¹‰æƒé™æšä¸¾
class Permission(str, Enum):
    AI_GENERATE = "ai:generate"
    AI_BATCH_GENERATE = "ai:batch_generate"
    AI_ADMIN = "ai:admin"

# æƒé™æ£€æŸ¥è£…é¥°å™¨
def require_permission(permission: Permission):
    def decorator(func):
        async def wrapper(*args, current_user: User, **kwargs):
            if not has_permission(current_user, permission):
                raise HTTPException(
                    status_code=403,
                    detail="æƒé™ä¸è¶³"
                )
            return await func(*args, current_user=current_user, **kwargs)
        return wrapper
    return decorator

# ä½¿ç”¨æƒé™æ§åˆ¶
@router.post("/api/ai/generate")
@require_permission(Permission.AI_GENERATE)
async def generate_content(...):
    pass
```

---

#### 9. å†…å®¹å®¡æ ¸æœºåˆ¶
```python
# æ•æ„Ÿè¯è¿‡æ»¤
SENSITIVE_WORDS = ["æ•æ„Ÿè¯1", "æ•æ„Ÿè¯2", ...]

def content_moderation(text: str) -> Tuple[bool, str]:
    """å†…å®¹å®¡æ ¸"""
    # 1. æ•æ„Ÿè¯æ£€æµ‹
    for word in SENSITIVE_WORDS:
        if word in text:
            return False, f"å†…å®¹åŒ…å«æ•æ„Ÿè¯: {word}"

    # 2. é•¿åº¦æ£€æŸ¥
    if len(text) > 100000:
        return False, "å†…å®¹è¿‡é•¿"

    # 3. è°ƒç”¨ç¬¬ä¸‰æ–¹å®¡æ ¸ APIï¼ˆå¯é€‰ï¼‰
    # result = moderation_api.check(text)

    return True, ""

# åœ¨ç”Ÿæˆååº”ç”¨
generated_content = ai_service.generate(prompt, config)
is_safe, reason = content_moderation(generated_content)
if not is_safe:
    raise HTTPException(400, detail=f"å†…å®¹å®¡æ ¸å¤±è´¥: {reason}")
```

---

## å››ã€é•¿æœŸä¼˜åŒ–å»ºè®®ï¼ˆ3-6ä¸ªæœˆï¼‰

### ğŸš€ æ€§èƒ½ä¼˜åŒ–

#### 10. æ•°æ®åº“æŒä¹…åŒ–æ¨¡æ¿
```python
# å°†æ¨¡æ¿å­˜å‚¨åˆ°æ•°æ®åº“
from sqlalchemy import Column, String, JSON, Boolean

class TemplateModel(Base):
    __tablename__ = "ai_templates"

    id = Column(String, primary_key=True)
    name = Column(String, nullable=False)
    description = Column(String)
    schema = Column(JSON)  # æ¨¡æ¿ç»“æ„
    prompt_template = Column(String)  # Jinja2 æ¨¡æ¿
    enabled = Column(Boolean, default=True)
    version = Column(String)
```

---

#### 11. AI æ¨¡å‹å¾®è°ƒ
```python
# ä½¿ç”¨ä¼ä¸šæ•°æ®å¾®è°ƒæ¨¡å‹
# 1. æ”¶é›†é«˜è´¨é‡ç”Ÿæˆæ ·æœ¬
# 2. æ„å»ºè®­ç»ƒæ•°æ®é›†
# 3. ä½¿ç”¨ OpenAI Fine-tuning API

import openai

# ä¸Šä¼ è®­ç»ƒæ•°æ®
openai.File.create(
    file=open("training_data.jsonl", "rb"),
    purpose="fine-tune"
)

# åˆ›å»ºå¾®è°ƒä»»åŠ¡
openai.FineTuningJob.create(
    training_file="file-xxx",
    model="gpt-4"
)

# ä½¿ç”¨å¾®è°ƒæ¨¡å‹
response = openai.ChatCompletion.create(
    model="ft:gpt-4:org-xxx",
    messages=[...]
)
```

---

#### 12. å¤šæ¨¡å‹è·¯ç”±ç­–ç•¥
```python
class ModelRouter:
    """æ™ºèƒ½æ¨¡å‹è·¯ç”±"""

    def select_model(self, template_id: str, section_id: str, data: dict) -> str:
        """æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©æ¨¡å‹"""

        # ç®€å•ä»»åŠ¡ -> ä½¿ç”¨å¿«é€Ÿä½æˆæœ¬æ¨¡å‹
        if self._is_simple_task(template_id, section_id):
            return "gpt-3.5-turbo"

        # å¤æ‚ä»»åŠ¡ -> ä½¿ç”¨é«˜è´¨é‡æ¨¡å‹
        if self._is_complex_task(template_id, section_id):
            return "gpt-4-turbo"

        # é»˜è®¤æ¨¡å‹
        return "gpt-4"

    def _is_simple_task(self, template_id, section_id) -> bool:
        # åˆ¤æ–­ä»»åŠ¡å¤æ‚åº¦
        simple_sections = ["1", "7", "8"]  # æ€»åˆ™ã€é™„å½•ç­‰
        return section_id in simple_sections
```

---

## äº”ã€å‰ç«¯é›†æˆå»ºè®®

### ğŸ¨ å‰ç«¯å¯¹æ¥ API

#### 1. API å®¢æˆ·ç«¯å°è£…
```typescript
// frontend/src/services/aiService.ts
import axios from 'axios';

interface GenerationRequest {
  template_id: string;
  section_id: string;
  data: Record<string, any>;
}

interface GenerationResponse {
  success: boolean;
  content?: string;
  section_title?: string;
  cached: boolean;
  error?: string;
}

export class AIService {
  private baseURL = '/api/ai';

  async listTemplates() {
    const response = await axios.get(`${this.baseURL}/templates`);
    return response.data;
  }

  async getTemplateSchema(templateId: string) {
    const response = await axios.get(`${this.baseURL}/templates/${templateId}/schema`);
    return response.data;
  }

  async generate(request: GenerationRequest): Promise<GenerationResponse> {
    const response = await axios.post(`${this.baseURL}/generate`, request);
    return response.data;
  }

  async streamGenerate(request: GenerationRequest, onChunk: (chunk: string) => void) {
    const response = await fetch(`${this.baseURL}/generate/stream`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(request)
    });

    const reader = response.body?.getReader();
    const decoder = new TextDecoder();

    while (true) {
      const { done, value } = await reader!.read();
      if (done) break;

      const chunk = decoder.decode(value);
      const lines = chunk.split('\n');

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = JSON.parse(line.slice(6));
          onChunk(data.content);
        }
      }
    }
  }
}
```

---

#### 2. React ç»„ä»¶ç¤ºä¾‹
```tsx
// frontend/src/components/AIGenerator.tsx
import { useState } from 'react';
import { AIService } from '../services/aiService';

export function AIGenerator({ documentId, sectionId }) {
  const [generating, setGenerating] = useState(false);
  const [content, setContent] = useState('');
  const aiService = new AIService();

  const handleGenerate = async () => {
    setGenerating(true);
    setContent('');

    try {
      // å‡†å¤‡æ•°æ®
      const data = {
        enterprise_info: {
          enterprise_name: "ç¤ºä¾‹ä¼ä¸š",
          industry_type: "åŒ–å·¥åˆ¶é€ ",
          // ...
        }
      };

      // æµå¼ç”Ÿæˆ
      await aiService.streamGenerate(
        {
          template_id: 'emergency_plan',
          section_id: sectionId,
          data
        },
        (chunk) => {
          setContent(prev => prev + chunk);
        }
      );
    } catch (error) {
      console.error('ç”Ÿæˆå¤±è´¥:', error);
    } finally {
      setGenerating(false);
    }
  };

  return (
    <div>
      <button onClick={handleGenerate} disabled={generating}>
        {generating ? 'AI ç”Ÿæˆä¸­...' : 'âœ¨ AI ç”Ÿæˆ'}
      </button>

      {content && (
        <div className="generated-content">
          {content}
        </div>
      )}
    </div>
  );
}
```

---

## å…­ã€æµ‹è¯•å»ºè®®

### ğŸ§ª å•å…ƒæµ‹è¯•
```python
# backend/tests/test_ai_generate.py
import pytest
from app.prompts.template_loader import TemplateLoader
from app.prompts.template_validator import TemplateValidator

def test_template_loader():
    loader = TemplateLoader()
    templates = loader.list_templates()
    assert len(templates) > 0
    assert templates[0]["id"] == "emergency_plan"

def test_template_validator():
    validator = TemplateValidator()

    # æµ‹è¯•æ­£å¸¸æ•°æ®
    data = {"enterprise_info": {"name": "æµ‹è¯•ä¼ä¸š"}}
    is_valid, errors = validator.validate_template_data(data)
    assert is_valid

    # æµ‹è¯•å±é™©æ•°æ®
    malicious_data = {"field": "{{__import__('os').system('ls')}}"}
    is_valid, errors = validator.validate_template_data(malicious_data)
    assert not is_valid
    assert len(errors) > 0

def test_ai_generate_api(client, auth_headers):
    response = client.post(
        "/api/ai/generate",
        json={
            "template_id": "emergency_plan",
            "section_id": "1",
            "data": {"enterprise_info": {"name": "æµ‹è¯•"}}
        },
        headers=auth_headers
    )
    assert response.status_code == 200
    assert response.json()["success"] == True
```

---

## ä¸ƒã€æ–‡æ¡£å»ºè®®

### ğŸ“š API æ–‡æ¡£
åœ¨ FastAPI è‡ªåŠ¨ç”Ÿæˆæ–‡æ¡£åŸºç¡€ä¸Šï¼Œæ·»åŠ è¯¦ç»†è¯´æ˜ï¼š

```python
@router.post(
    "/api/ai/generate",
    response_model=GenerationResponse,
    summary="ç”Ÿæˆ AI å†…å®¹",
    description="""
    æ ¹æ®æ¨¡æ¿å’Œæ•°æ®ç”Ÿæˆ AI å†…å®¹

    **ä½¿ç”¨æ­¥éª¤ï¼š**
    1. è°ƒç”¨ /api/ai/templates è·å–å¯ç”¨æ¨¡æ¿åˆ—è¡¨
    2. è°ƒç”¨ /api/ai/templates/{id}/schema è·å–æ¨¡æ¿ç»“æ„
    3. å‡†å¤‡æ•°æ®å¹¶è°ƒç”¨æœ¬æ¥å£ç”Ÿæˆå†…å®¹

    **é™æµè¯´æ˜ï¼š**
    - 1åˆ†é’Ÿå†…æœ€å¤š 10 æ¬¡è¯·æ±‚
    - 1å°æ—¶å†…æœ€å¤š 100 æ¬¡è¯·æ±‚

    **ç¼“å­˜æœºåˆ¶ï¼š**
    - ç›¸åŒè¾“å…¥ä¼šè¿”å›ç¼“å­˜ç»“æœ
    - ç¼“å­˜æœ‰æ•ˆæœŸ 1 å°æ—¶
    """,
    responses={
        200: {"description": "ç”ŸæˆæˆåŠŸ"},
        400: {"description": "æ•°æ®éªŒè¯å¤±è´¥"},
        429: {"description": "è¯·æ±‚è¿‡äºé¢‘ç¹"},
        500: {"description": "æœåŠ¡å™¨é”™è¯¯"}
    }
)
async def generate_content(...):
    pass
```

---

## å…«ã€éƒ¨ç½²å»ºè®®

### ğŸš¢ Docker åŒ–éƒ¨ç½²

#### Dockerfile
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# ç¯å¢ƒå˜é‡
ENV PYTHONUNBUFFERED=1
ENV LOG_LEVEL=INFO

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=3s \
  CMD curl -f http://localhost:8000/health || exit 1

# å¯åŠ¨æœåŠ¡
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### docker-compose.yml
```yaml
version: '3.8'

services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:pass@db:5432/yueen
      - REDIS_URL=redis://redis:6379/0
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - db
      - redis

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  db:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=yueen
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
    volumes:
      - db_data:/var/lib/postgresql/data

volumes:
  redis_data:
  db_data:
```

---

## ä¹ã€æˆæœ¬ä¼˜åŒ–

### ğŸ’° AI API æˆæœ¬æ§åˆ¶

1. **æ¨¡å‹é€‰æ‹©ç­–ç•¥**
   - ç®€å•ä»»åŠ¡ï¼šgpt-3.5-turbo ($0.001/1K tokens)
   - å¤æ‚ä»»åŠ¡ï¼šgpt-4-turbo ($0.01/1K tokens)

2. **ç¼“å­˜ä¼˜åŒ–**
   - æé«˜ç¼“å­˜å‘½ä¸­ç‡ï¼Œå‡å°‘é‡å¤è¯·æ±‚
   - ç¼“å­˜çƒ­é—¨ç« èŠ‚å†…å®¹

3. **æ‰¹é‡å¤„ç†**
   - åˆå¹¶å¤šä¸ªå°è¯·æ±‚ä¸ºä¸€ä¸ªå¤§è¯·æ±‚
   - å‡å°‘ API è°ƒç”¨æ¬¡æ•°

4. **Token æ§åˆ¶**
   - é™åˆ¶ max_tokens é¿å…è¿‡é•¿è¾“å‡º
   - ä¼˜åŒ– Prompt é•¿åº¦

---

## åã€æ€»ç»“

### âœ… ç«‹å³å¯åšï¼ˆæœ¬å‘¨ï¼‰
1. æ¥å…¥çœŸå® AI æ¨¡å‹
2. é›†æˆ Redis ç¼“å­˜
3. æ·»åŠ é‡è¯•æœºåˆ¶
4. å®Œå–„é”™è¯¯æ—¥å¿—

### ğŸ”œ è¿‘æœŸè®¡åˆ’ï¼ˆæœ¬æœˆï¼‰
1. å®ç°æµå¼å“åº”
2. æ‰¹é‡ç”Ÿæˆæ¥å£
3. å®¡è®¡æ—¥å¿—å®Œå–„
4. Prometheus ç›‘æ§

### ğŸ¯ é•¿æœŸè§„åˆ’ï¼ˆ3-6ä¸ªæœˆï¼‰
1. æ¨¡å‹å¾®è°ƒ
2. å¤šæ¨¡æ¿æ‰©å±•
3. æ€§èƒ½ä¼˜åŒ–
4. æˆæœ¬ä¼˜åŒ–

å½“å‰ç³»ç»Ÿå·²å…·å¤‡ç”Ÿäº§ç¯å¢ƒåŸºç¡€ï¼Œå®Œæˆä¸Šè¿°ä¼˜åŒ–åå¯ä»¥å®‰å…¨ã€é«˜æ•ˆåœ°æŠ•å…¥ä½¿ç”¨ã€‚
